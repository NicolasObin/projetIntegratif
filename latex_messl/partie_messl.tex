\documentclass[a4paper,12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[frenchb]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage[babel]{csquotes}
\MakeAutoQuote{«}{»}
\pagestyle{headings}

\title{Notes sur MESSL}
\author{jmatthieu}

\begin{document}
\maketitle
Les notations sont les mêmes sauf lorsque précisé. Les équations du papier MESSL sont notées (m$N$) où $N$ correspond au numéro de l'équation dans le papier MESSL.

\section{Log-vraisemblance du modèle}

Le modèle (noté $\Lambda$ au lieu de $\Theta$) que l'on veut estimer a pour paramètres (voir \textit{III. Parameter estimation from mixtures} et l'équation (m10) pour le détail) :
\begin{equation*}
\Lambda = \{\xi(\omega),\sigma_{i,\tau}(\omega),\mu_{i}(\omega),\nu_i(\omega),\psi_{i,\tau}\}
\end{equation*}
La probabilité de l'observation $\phi(\omega,t) ; \alpha(\omega,t)$ connaissant les paramètres du modèle $\Lambda$ est donnée par :
\begin{equation*}
p(\phi(\omega,t), \alpha(\omega,t) | \Lambda)
\end{equation*}
La vraisemblance est donnée par (définition) :
\begin{align*}
L(\Lambda) = \displaystyle \prod_{\omega,t} p(\phi(\omega,t), \alpha(\omega,t) | \Lambda)
\end{align*}
Et la log-vraisemblance par (définition) :
\begin{align}
\mathcal{L}(\Lambda) = & \log L(\Lambda) \nonumber \\
 = & \log \displaystyle \prod_{\omega,t} p(\phi(\omega,t), \alpha(\omega,t) | \Lambda) \nonumber \\
 = & \displaystyle\sum_{\omega,t} \log p(\phi(\omega,t), \alpha(\omega,t) | \Lambda) \label{eq:logvrai}
\end{align}
où l'équation (\ref{eq:logvrai}) correspond à l'équation (m12).

La variable cachée $z_{i,\tau}(\omega,t)$ correspond à la classe de chaque observation, c'est à dire au couple ($i$; $\tau$) dont est issue l'observation. $z_{i,\tau}(\omega,t) = 1$ si l'observation est issue du couple ($i$; $\tau$), zéro autrement. En marginalisant sur la variable cachée $z_{i,\tau}(\omega,t)$ puis en appliquant la loi de Bayes on obtient à partir de l'équation (1)
\begin{align}
& p(\phi(\omega,t), \alpha(\omega,t) | \Lambda)\nonumber \\
= & \displaystyle\sum_{i, \tau} p(\phi(\omega,t), \alpha(\omega,t), z_{i, \tau}(\omega,t) | \Lambda) \nonumber\\
= & \displaystyle\sum_{i, \tau} p(\phi(\omega,t), \alpha(\omega,t) | z_{i, \tau}(\omega,t), \Lambda) \cdot p(z_{i, \tau}(\omega,t) | \Lambda) \label{eq:margi}
\end{align}

En injectant (\ref{eq:margi}) dans (\ref{eq:logvrai}) on obtient :
\begin{align}
 \mathcal{L}(\Lambda) = \displaystyle\sum_{\omega,t} \log \displaystyle\sum_{i, \tau} p(\phi(\omega,t), \alpha(\omega,t) | z_{i, \tau}(\omega,t), \Lambda) \cdot p(z_{i, \tau}(\omega,t) | \Lambda) \label{eq:logvmargi}
\end{align}
On suppose $\phi$ et $\alpha$ indépendants, et on modélise leur probabilité connaissant $z_{i,\tau}$ et $\Lambda$ par une gaussienne, on a donc :
\begin{align}
& p(\phi(\omega,t), \alpha(\omega,t) | z_{i, \tau}(\omega,t), \Lambda) \nonumber\\
= & p(\phi(\omega,t) | z_{i, \tau}(\omega,t), \Lambda) \cdot p(\alpha(\omega,t) | z_{i, \tau}(\omega,t), \Lambda) \nonumber\\
= & \mathcal{N}(\hat{\phi}(\omega,t;\tau) | \xi_{i,\tau}(\omega),\sigma^{2}_{i,\tau}(\omega)) \cdot \mathcal{N}(\alpha(\omega,t;\tau) | \mu_{i,\tau}(\omega),\eta^{2}_{i,\tau}(\omega)) \label{eq:modgauss}
\end{align}
On pose :
\begin{equation}
\psi_{i,\tau} \equiv  p(z_{i, \tau}(\omega,t) | \Lambda) \label{eq:psi}
\end{equation}
Et on retombe sur (m13) en injectant (\ref{eq:modgauss}) et (\ref{eq:psi}) dans (\ref{eq:logvmargi})


\section{Fonction auxiliaire $Q$}

Pour alléger la notation dans cette section nous écrirons $\phi(\omega,t)$, $\alpha(\omega,t)$ et $z_{i, \tau}$ sans leurs dépendances en $(\omega,t)$.

On cherche la différence entre la logvraisemblance actuelle et celle calculée au pas précédent $s$.
\begin{align}
& \mathcal{L}(\Lambda) - \mathcal{L}(\Lambda_s) & \nonumber \\
= & \displaystyle\sum_{\omega,t} \log \displaystyle\sum_{i, \tau} p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda) - \displaystyle\sum_{\omega,t} \log p(\phi, \alpha | \Lambda_s) \nonumber\\
= & \displaystyle\sum_{\omega,t} \log \displaystyle\sum_{i, \tau} p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda) \times \frac{p(z|\phi,\alpha,\Lambda_s)}{p(z|\phi,\alpha,\Lambda_s)}- \displaystyle\sum_{\omega,t} \log p(\phi, \alpha | \Lambda_s) \nonumber\\
= & \displaystyle\sum_{\omega,t} \log \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \times \frac{p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda)}{p(z|\phi,\alpha,\Lambda_s)}- \displaystyle\sum_{\omega,t} \log p(\phi, \alpha | \Lambda_s) \nonumber\\
\geq & \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \times \log \frac{p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda)}{p(z|\phi,\alpha,\Lambda_s)}- \displaystyle\sum_{\omega,t} \log p(\phi, \alpha | \Lambda_s) \label{eq:jensen}
\end{align}
Le passage à (\ref{eq:jensen}) est l'application de la formule de Jensen, car
\begin{equation*}
\displaystyle \sum_{i,\tau} p(z|\phi,\alpha,\Lambda_s) = 1
\end{equation*}
ce qui permet également de factoriser (\ref{eq:jensen}) pour obtenir :
\begin{align}
& \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \times \log \frac{p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda)}{p(z|\phi,\alpha,\Lambda_s)}- \displaystyle\sum_{\omega,t}\displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \log p(\phi, \alpha | \Lambda_s) \nonumber \\
= & \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \times [ \log \frac{p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda)}{p(z|\phi,\alpha,\Lambda_s)}-\log p(\phi, \alpha | \Lambda_s) ] \nonumber \\
= & \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \times \log \frac{p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda)}{p(z|\phi,\alpha,\Lambda_s) \cdot p(\phi, \alpha | \Lambda_s)}  \label{avantk} \\
= & \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \times [\log p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda) - \log p(z|\phi,\alpha,\Lambda_s) \cdot p(\phi, \alpha | \Lambda_s)] \nonumber
\end{align}
et si on regroupe ce qui ne dépend pas de $\Lambda$ sous le terme $k$ on obtient :
\begin{equation*}
k + \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \cdot \log p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda)
\end{equation*}
puis en appliquant la loi de Bayes à l'intérieur du log :
\begin{equation*}
k + \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \cdot \log p(\phi, \alpha, z_{i, \tau} | \Lambda)
\end{equation*}
on retouve bien l'équation (m14) :
\begin{equation}
Q(\Lambda|\Lambda_s) = k + \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \cdot \log p(\phi, \alpha, z_{i, \tau} | \Lambda) \label{Q}
\end{equation}

\section{E-step}
%D'après la loi de Bayes :
%\begin{equation*}
%p(\phi, \alpha, z_{i, \tau} | \Lambda) = p(\phi,\alpha|\Lambda) \cdot p(z_{i,\tau} | \phi, \alpha, \Lambda)
%\end{equation*}
D'après (\ref{avantk}) :
\begin{align}
Q(\Lambda|\Lambda_s) & = \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \times \log \frac{p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda)}{p(z|\phi,\alpha,\Lambda_s) \cdot p(\phi, \alpha | \Lambda_s)} \nonumber \\
& = \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \times [ \log \frac{p(\phi, \alpha | z_{i, \tau}, \Lambda) \cdot p(z_{i, \tau} | \Lambda)}{p(z|\phi,\alpha,\Lambda_s)}  - \log p(\phi, \alpha | \Lambda_s) ] \nonumber \\
& = \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \times [ \log \frac{p(\phi, \alpha, z_{i, \tau} | \Lambda)}{p(z|\phi,\alpha,\Lambda_s)}  - \log p(\phi, \alpha | \Lambda_s) ] \nonumber \\
& = k_1 + \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} p(z|\phi,\alpha,\Lambda_s) \times \log \frac{p(\phi, \alpha, z_{i, \tau} | \Lambda)}{p(z|\phi,\alpha,\Lambda_s)} \nonumber
\end{align}
avec $k_1$ qui ne dépend pas de $\Lambda$. Si on ne s'intéresse qu'au second terme et qu'on introduit l'espérance (on a reconnu $\nu_{i,\tau}$) :
\begin{align*}
& \displaystyle\sum_{\omega,t} \mathbb{E}_{\nu_{i,\tau}} [ \log \frac{p(\phi, \alpha, z_{i, \tau} | \Lambda)}{\nu_{i,\tau}}] \\
= & \displaystyle\sum_{\omega,t} \mathbb{E}_{\nu_{i,\tau}} [ \log \frac{p(\phi, \alpha | \Lambda) \cdot p(z_{i, \tau} |\phi, \alpha, \Lambda)}{\nu_{i,\tau}}] \\
= & \displaystyle\sum_{\omega,t} \mathbb{E}_{\nu_{i,\tau}} [ \log \frac{p(\phi, \alpha | \Lambda) \cdot p(z_{i, \tau} |\phi, \alpha, \Lambda)}{\nu_{i,\tau}}] \\
= & \displaystyle\sum_{\omega,t} \mathbb{E}_{\nu_{i,\tau}} [\log p(\phi, \alpha | \Lambda) + \log \frac{p(z_{i, \tau} |\phi, \alpha, \Lambda)}{\nu_{i,\tau}}]
\end{align*}
Comme le premier log ne dépend pas de $z_{i,\tau}$ :
\begin{align*}
& \displaystyle\sum_{\omega,t} \log p(\phi, \alpha | \Lambda) + \mathbb{E}_{\nu_{i,\tau}} [\log \frac{p(z_{i, \tau} |\phi, \alpha, \Lambda)}{\nu_{i,\tau}}] \\
= & \displaystyle\sum_{\omega,t} \log p(\phi, \alpha | \Lambda) - \mathbb{E}_{\nu_{i,\tau}} [\log \frac{\nu_{i,\tau}}{p(z_{i, \tau} |\phi, \alpha, \Lambda)}] \\
\end{align*}
Le second terme ressemble fortement à une divergence KL, donc pour maximiser le tout il faut que l'espérance soit la plus petite possible ce qui est obtenu pour :
\begin{equation}
\hat\nu_{i,\tau} \leftarrow p(z_{i, \tau} |\phi, \alpha, \Lambda) \label{Estep}
\end{equation}
Et on comprend pourquoi (m15).

\section{M-step}
On veut désormais maximiser $Q(\Lambda|\Lambda_s)$ par rapport à $\Lambda$, et l'équation (m15) pose :
\begin{equation*}
\nu_{i,\tau} \equiv p(z|\phi,\alpha,\Lambda_s)
\end{equation*}
Ce qui donne en injectant dans (\ref{Q}) et en écrivant ensuite l'espérance :
\begin{align}
Q(\Lambda|\Lambda_s) & = k + \displaystyle\sum_{\omega,t} \displaystyle\sum_{i, \tau} \nu_{i,\tau} \cdot \log p(\phi, \alpha, z_{i, \tau} | \Lambda) \nonumber \\
& = k + \displaystyle\sum_{\omega,t} \mathbb{E}_{\nu_{i,\tau}} [\log p(\phi, \alpha, z_{i, \tau} | \Lambda)] \label{Qexp}
\end{align}
On rappelle que $k$ ne dépend pas de $\Lambda$, on peut donc l'exclure de la maximisation par rapport à $\Lambda$, et ainsi :
\begin{equation*}
\hat\Lambda \leftarrow argmax_\Lambda \; \displaystyle\sum_{\omega,t} \mathbb{E}_{\nu_{i,\tau}} [\log p(\phi, \alpha, z_{i, \tau} | \Lambda)]
\end{equation*}
%\begin{align}
%\nu_{i,\tau} & \equiv p(z|\phi,\alpha,\Lambda_s) \nonumber \\
%& \propto p(z,\phi,\alpha|\Lambda_s) \nonumber \\
%& = p(z| \Lambda_s) \cdot p(\phi | \Lambda_s) \cdot p(\alpha| \Lambda_s) \nonumber \\
%& = \psi_{i,\tau} \cdot \mathcal{N}(\hat{\phi}(\omega,t;\tau) | \xi_{i,\tau}(\omega),\sigma^{2}_{i,\tau}(\omega)) \cdot \mathcal{N}(\alpha(\omega,t;\tau) | \mu_{i,\tau}(\omega),\eta^{2}_{i,\tau}(\omega)) \nonumber
%\end{align}
%La variable cachée binaire $z_{i,\tau}$ a pour espérance connaissant $\phi,\alpha,\Lambda_s$ :
%\begin{align}
%E_{\phi,\alpha,\Lambda_s}[z_{i,\tau}] & = p(z_{i,\tau} = 1 | \phi,\alpha,\Lambda_s) \times 1 + p(z_{i,\tau} = 0 | \phi,\alpha,\Lambda_s) \times 0 \nonumber \\
%& = p(z_{i,\tau} = 1 | \phi,\alpha,\Lambda_s) \nonumber
%\end{align}
%\textrightarrow le calcul de $\nu_{i,\tau}$ donne l'espérance de $z_{i,\tau}$

\end{document}